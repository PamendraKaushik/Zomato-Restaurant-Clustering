{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "H0kj-8xxnORC",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Unsupervised ML - Zomato Restaurant Clustering**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Pamendra Kaushik\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project performs Unsupervised Machine Learning on Zomato restaurant data to identify meaningful customer–restaurant patterns and segment restaurants into similar groups. The workflow begins with loading and exploring two datasets containing restaurant metadata and customer reviews. After handling duplicates, missing values, and inconsistent data formats, important features such as cost, cuisines, types, collections, review text length, and picture count were engineered to create a structured dataset for modelling.\n",
        "\n",
        "Multiple clustering algorithms such as K-Means, Agglomerative (Hierarchical) Clustering, and DBSCAN were applied and evaluated. Metrics like the Silhouette Score were used to compare model performance. Based on the scores and cluster stability, the best-fit clustering solution was selected to group restaurants into distinct segments such as premium dining, budget-friendly outlets, and high-engagement restaurants.\n",
        "\n",
        "These clusters help in understanding restaurant behaviour, customer preferences, and business positioning. The results can be used for improving recommendations, better categorisation on food platforms, and providing data-driven insights for restaurant owners."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zomato hosts thousands of restaurants with diverse cuisines, pricing, customer reviews, and popularity levels. Understanding these restaurants and grouping similar ones together is challenging due to the high variation in features such as cost, cuisines, ratings, and review behaviour.\n",
        "Traditional supervised learning cannot be applied because the dataset does not contain predefined labels or categories.\n",
        "\n",
        "The objective of this project is to apply Unsupervised Machine Learning techniques to cluster restaurants into meaningful groups based on their metadata and review characteristics. By analysing numerical and text-derived features, the project aims to identify hidden patterns and segments such as premium restaurants, mid-range popular outlets, low-engagement restaurants, and budget-friendly options.\n",
        "\n",
        "These insights help food platforms like Zomato improve recommendations, help customers discover restaurants more easily, and allow business owners to understand their competitive positioning using data-driven clusters."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score, pairwise_distances\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from nltk.corpus import stopwords\n",
        "import datetime\n",
        "import ast\n",
        "import joblib\n",
        "sns.set(style=\"whitegrid\")\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "meta = pd.read_csv('/content/Zomato Restaurant names and Metadata.csv')\n",
        "reviews = pd.read_csv('/content/Zomato Restaurant reviews.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta.head(5)"
      ],
      "metadata": {
        "id": "PUhWsxiSoTpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews.head(5)"
      ],
      "metadata": {
        "id": "zMqFQC-xoYHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta.shape"
      ],
      "metadata": {
        "id": "s3SLomtNo61n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews.shape"
      ],
      "metadata": {
        "id": "yqxnii4Qo67s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta.info()"
      ],
      "metadata": {
        "id": "XlNgBwiKp1o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews.info()"
      ],
      "metadata": {
        "id": "1F8KTVtFp1sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(meta.duplicated().sum())\n",
        "print(reviews.duplicated().sum())"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(meta.isnull().sum())\n",
        "print(reviews.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "meta.isnull().sum().plot(kind='bar')"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews.isnull().sum().plot(kind='bar')"
      ],
      "metadata": {
        "id": "DnQhUK66qSew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Rating has non-numeric values like \"Like\"\n",
        "“Like” → np.nan OR maybe map to a default (we can decide)\n",
        "\n",
        "2. Metadata has multiple patterns\n",
        "\n",
        "Example:\n",
        "\n",
        "\"3 Reviews , 2 Followers\"\n",
        "\n",
        "\"1 Review\"\n",
        "\n",
        "\"30 Reviews , 34 Followers\"\n",
        "\n",
        " 3. 36 duplicate rows in reviews"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(meta.columns)\n",
        "print(reviews.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "meta.describe(include='all')\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews.describe(include='all')"
      ],
      "metadata": {
        "id": "eDMlKmdTtbcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Meta Data:**\n",
        "\n",
        "Name: Name of Restaurants\n",
        "\n",
        "Links: URL Links of Restaurants\n",
        "\n",
        "Cost: Per person estimated cost of dining\n",
        "\n",
        "Collection: Tagging of Restaurants w.r.t. Zomato categories\n",
        "\n",
        "Cuisines: Cuisines served by restaurants\n",
        "\n",
        "Timings:Restaurant  timings                                          \n",
        "  \n",
        "**Review Data:**\n",
        "\n",
        "Reviewer: Name of the reviewer\n",
        "\n",
        "Review: Review text\n",
        "\n",
        "Rating: Rating provided\n",
        "\n",
        "MetaData: Reviewer metadata - Number of reviews and followers\n",
        "\n",
        "Time: Date and Time of Review\n",
        "\n",
        "Pictures: Number of pictures posted with review"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta.nunique()"
      ],
      "metadata": {
        "id": "gK_pocIuu2Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews.nunique()"
      ],
      "metadata": {
        "id": "z98RIi6_u9Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Clean \"Cost\" column in metadata → numeric (Cost_clean)\n",
        "meta[\"Cost_clean\"] = (\n",
        "    meta[\"Cost\"]              # e.g. \"1,300\"\n",
        "    .astype(str)                 # ensure string\n",
        "    .str.replace(\",\", \"\")        # remove comma\n",
        "    .str.extract(r\"(\\d+)\", expand=False)  # keep only digits\n",
        "    .astype(float)               # convert to float\n",
        ")"
      ],
      "metadata": {
        "id": "hEwJ_Tp8DM3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2️⃣ Clean \"Rating\" in reviews → numeric\n",
        "reviews[\"Rating\"] = pd.to_numeric(reviews[\"Rating\"], errors=\"coerce\")"
      ],
      "metadata": {
        "id": "RZ0U8JWRz_2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3️⃣ Convert \"Time\" in reviews → datetime (optional but good)\n",
        "reviews[\"Time\"] = pd.to_datetime(reviews[\"Time\"], errors=\"coerce\")"
      ],
      "metadata": {
        "id": "UrG9kkOO0WJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4️⃣ Drop rows with missing critical fields\n",
        "#    - Restaurant name or Rating missing in reviews → drop\n",
        "#    - Restaurant Name missing in meta → drop\n",
        "reviews = reviews.dropna(subset=[\"Restaurant\", \"Rating\"])\n",
        "meta = meta.dropna(subset=[\"Name\"])\n",
        "\n",
        "print(\"After basic cleaning:\")\n",
        "print(\"  Metadata shape :\", meta.shape)\n",
        "print(\"  Reviews shape  :\", reviews.shape)"
      ],
      "metadata": {
        "id": "TlBBbQ6XNacX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5️⃣ Check remaining missing values (for info)\n",
        "print(\"\\nMissing values in meta_df:\")\n",
        "print(meta.isna().sum())\n",
        "\n",
        "print(\"\\nMissing values in reviews_df:\")\n",
        "print(reviews.isna().sum())\n"
      ],
      "metadata": {
        "id": "QopcEcDnNala"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6️⃣ Aggregate reviews at restaurant level\n",
        "#    We create one row per restaurant with:\n",
        "#    - avg_rating       : mean rating\n",
        "#    - review_count     : number of reviews\n",
        "#    - total_pictures   : total pictures uploaded\n",
        "#    - avg_review_length: average length of review text\n",
        "restaurant_agg = (\n",
        "    reviews\n",
        "    .groupby(\"Restaurant\")\n",
        "    .agg(\n",
        "        avg_rating=(\"Rating\", \"mean\"),\n",
        "        review_count=(\"Rating\", \"count\"),\n",
        "        total_pictures=(\"Pictures\", \"sum\"),\n",
        "        avg_review_length=(\"Review\", lambda x: x.astype(str).str.len().mean())\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "print(\"\\nRestaurant-level aggregated reviews (first 5 rows):\")\n",
        "display(restaurant_agg.head())"
      ],
      "metadata": {
        "id": "2qQoVgnnNaqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7️⃣ Merge aggregated reviews with restaurant metadata\n",
        "#    match Restaurant (reviews) with Name (metadata)\n",
        "restaurants_full = pd.merge(\n",
        "    restaurant_agg,\n",
        "    meta,\n",
        "    left_on=\"Restaurant\",\n",
        "    right_on=\"Name\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "print(\"\\nMerged restaurant dataset shape:\", restaurants_full.shape)\n",
        "display(restaurants_full.head())\n"
      ],
      "metadata": {
        "id": "yWXhO18mNauz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restaurants_full.columns"
      ],
      "metadata": {
        "id": "4H2_JyurQFNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropped missing values and duplicates\n",
        "Extracted cuisines from the Cuisines column\n",
        "Converted cost column to int data type\n",
        "Insights\n",
        "\n",
        "There are 44 unique cuisines across 104 restaurants\n",
        "Estimated cost of dining of all 104 restaurents are in the range 150 Rs to 2800 Rs\n",
        "Extracting the locations from the links column we can observe that all restaurents are from Gachibowli, Hyderabad"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 **Distribution of Cost Across Different Cuisines (Box Plot)**"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "restaurants_full[\"primary_cuisine\"] = restaurants_full[\"Cuisines\"].fillna(\"Unknown\").apply(lambda x: str(x).split(\",\")[0])\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(\n",
        "    data=restaurants_full,\n",
        "    x=\"primary_cuisine\",\n",
        "    y=\"Cost_clean\"\n",
        ")\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Distribution of Cost Across Different Cuisines\")\n",
        "plt.xlabel(\"Cuisine\")\n",
        "plt.ylabel(\"Cost for Two (₹)\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A box plot is ideal for comparing price distributions across many cuisines — it shows median, spread, and outliers clearly."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continental, Café, Italian have highest median prices.\n",
        "\n",
        "North Indian, Fast Food, Chinese show huge variation → some very cheap, some premium.\n",
        "\n",
        "Dessert/Bakery have consistently low cost."
      ],
      "metadata": {
        "id": "YZWTSL2RTyiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive:**\n",
        "Helps Zomato build “Affordable Cuisine” vs “Premium Cuisine” categories.\n",
        "Identifies cuisines with stable pricing → easier to promote.\n",
        "\n",
        "**Negative Insight:**\n",
        "Cuisines like North Indian/Chinese show inconsistent pricing, hurting customer trust."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - **Bar Chart: Most Popular Cuisines (Total Reviews)**"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "popular = (\n",
        "    restaurants_full.groupby(\"primary_cuisine\")[\"review_count\"]\n",
        "    .sum()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(15)\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "popular.plot(kind=\"bar\", color=\"skyblue\")\n",
        "plt.title(\"Most Popular Cuisines (Based on Total Reviews)\")\n",
        "plt.xlabel(\"Cuisine\")\n",
        "plt.ylabel(\"Total Reviews\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart clearly compares total popularity (review count) across cuisines."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "North Indian and Chinese are the most popular cuisines.\n",
        "\n",
        "Fast Food, Desserts, Bakery also show high review volume.\n",
        "\n",
        "Premium cuisines (Continental/Italian) have fewer restaurants but high reviews per outlet."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive:**\n",
        "Helps Zomato target promotions toward highly-engaged cuisines.\n",
        "Nice cuisines with high review density are great for premium campaigns.\n",
        "\n",
        "**Negative Insight:**\n",
        " Popular cuisines are oversaturated → lower margins, tougher competition."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 **Scatter Plot: Cost vs Rating (Bubble = Review Count)**"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(\n",
        "    data=restaurants_full,\n",
        "    x=\"Cost_clean\",\n",
        "    y=\"avg_rating\",\n",
        "    size=\"review_count\",\n",
        "    hue=\"primary_cuisine\",\n",
        "    sizes=(20,200),\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.title(\"Cost vs Rating (Bubble Size = Review Count)\")\n",
        "plt.xlabel(\"Cost for Two (₹)\")\n",
        "plt.ylabel(\"Average Rating\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows the relationship between cost and quality (rating), while bubble size shows popularity."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost and rating have almost no correlation.\n",
        "\n",
        "Many high-rated restaurants are affordable (₹300–₹600).\n",
        "\n",
        "Expensive restaurants (>₹1000) often have average ratings → poor value."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive:**\n",
        "Highlights value-for-money restaurants, ideal for recommendations.\n",
        "Helps premium restaurants identify quality issues.\n",
        "\n",
        "**Negative Insight:**\n",
        "Expensive restaurants with poor ratings harm Zomato’s brand reliability."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 **Pie Chart: Cuisine Market Share**"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cuisine_counts = restaurants_full[\"primary_cuisine\"].value_counts().head(8)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(\n",
        "    cuisine_counts,\n",
        "    labels=cuisine_counts.index,\n",
        "    autopct=\"%1.1f%%\",\n",
        "    startangle=90\n",
        ")\n",
        "plt.title(\"Cuisine Market Share (Top 8 Cuisines)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pie chart shows proportional distribution quickly and visually."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "North Indian & Chinese dominate market share.\n",
        "\n",
        "Niche cuisines have <5% share."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive:**\n",
        "Helps with cuisine-level planning for campaigns & promotions.\n",
        "\n",
        "**Negative Insight:**\n",
        "Heavy reliance on two cuisines creates low variety → possible customer fatigue."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 **Line Chart: Average Cost Across Rating Buckets**"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "restaurants_full[\"rating_bucket\"] = pd.cut(\n",
        "    restaurants_full[\"avg_rating\"],\n",
        "    bins=[0,2.5,3,3.5,4,4.5,5],\n",
        "    labels=[\"0-2.5\",\"2.5-3\",\"3-3.5\",\"3.5-4\",\"4-4.5\",\"4.5-5\"]\n",
        ")\n",
        "\n",
        "avg_cost_bucket = (\n",
        "    restaurants_full.groupby(\"rating_bucket\")[\"Cost_clean\"]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.lineplot(data=avg_cost_bucket, x=\"rating_bucket\", y=\"Cost_clean\", marker=\"o\")\n",
        "plt.title(\"Cost Trend Across Rating Groups\")\n",
        "plt.xlabel(\"Rating Range\")\n",
        "plt.ylabel(\"Average Cost (₹)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line chart helps detect trends in price as rating increases."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Highest-rated restaurants (4.5–5) don’t always have high cost.\n",
        "\n",
        "Mid-rated restaurants (3.5–4.5) show the best price–rating balance."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive:**\n",
        "Helps Zomato push “Top Rated Under ₹500” collections.\n",
        "Useful for value-based recommendations.\n",
        "\n",
        "**Negative Insight:**\n",
        " Very high-rated restaurants fluctuate in cost → lack pricing consistency."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Chart - 6 **Horizontal Bar Chart: Most Popular Restaurants**"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_restaurants = restaurants_full.sort_values(\"review_count\", ascending=False).head(20)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.barplot(\n",
        "    data=top_restaurants,\n",
        "    x=\"review_count\",\n",
        "    y=\"Restaurant\",\n",
        "    hue=\"primary_cuisine\"\n",
        ")\n",
        "plt.title(\"Top 20 Most Popular Restaurants\")\n",
        "plt.xlabel(\"Review Count\")\n",
        "plt.ylabel(\"Restaurant\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideal for ranking individual restaurants; horizontal bars accommodate long names."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most top restaurants belong to North Indian, Chinese, Fast Food.\n",
        "\n",
        "A few restaurants dominate review volume → brand loyalty."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive:**\n",
        "Zomato can offer exclusive deals with high-traffic restaurants.\n",
        "Strong candidates for “Zomato Recommended”.\n",
        "\n",
        "**Negative:**\n",
        "Smaller restaurants don’t get visibility → platform inequality."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 **Heatmap: Correlation of Cost, Rating, Reviews**"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(\n",
        "    restaurants_full[[\"Cost_clean\", \"avg_rating\", \"review_count\"]].corr(),\n",
        "    annot=True,\n",
        "    cmap=\"coolwarm\"\n",
        ")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provides exact numerical correlation values that scatter plots only visually hint at."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost vs Rating ≈ 0 → No relationship.\n",
        "\n",
        "Cost vs Review Count = negative correlation → expensive restaurants get fewer reviews.\n",
        "\n",
        "Rating vs Review Count = slight positive."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive:**\n",
        "Irrefutably proves that price ≠ quality.\n",
        "Helps Zomato design evidence-based clustering & recommendation.\n",
        "\n",
        "**Negative Insight:**\n",
        "High-cost restaurants getting fewer reviews = low customer engagement risk."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.“Average cost differs significantly across different cuisines.”\n",
        "\n",
        "2.“More expensive restaurants receive higher ratings.”\n",
        "\n",
        "3.“Popular restaurants (high review count) have higher ratings.”"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“More expensive restaurants receive higher ratings.”\n",
        "\n",
        "This came from Chart 3 (Cost vs Rating scatter plot) and Heatmap.\n",
        "\n",
        "Null & Alternate Hypothesis Null Hypothesis (H₀):\n",
        "There is no correlation between restaurant cost and rating. (cost and rating are independent)\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "\n",
        "There is a correlation between restaurant cost and rating. (cost influences rating)"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# create groups for cuisines with enough samples\n",
        "groups = [\n",
        "    grp[\"Cost_clean\"].dropna().values\n",
        "    for name, grp in restaurants_full.groupby(\"primary_cuisine\")\n",
        "    if len(grp) > 5\n",
        "]\n",
        "\n",
        "f_stat, p_value = f_oneway(*groups)\n",
        "f_stat, p_value\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Type of test used:\n",
        "\n",
        "One-Way ANOVA (Analysis of Variance)\n",
        "\n",
        "Why ANOVA?\n",
        "\n",
        "We are comparing the means of more than two groups (many cuisines).\n",
        "\n",
        "Cost is a continuous variable.\n",
        "\n",
        "Cuisines are categorical groups.\n",
        "\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ANOVA is the correct test for comparing the mean of a continuous variable across multiple categories."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“More expensive restaurants receive higher ratings.”\n",
        "\n",
        "This came from Chart 3 (Cost vs Rating scatter plot) and Heatmap.\n",
        "\n",
        "1. Null & Alternate Hypothesis\n",
        "Null Hypothesis (H₀):\n",
        "\n",
        "There is no correlation between restaurant cost and rating.\n",
        "(cost and rating are independent)\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "\n",
        "There is a correlation between restaurant cost and rating.\n",
        "(cost influences rating)"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "corr, p_value = pearsonr(restaurants_full[\"Cost_clean\"].dropna(), restaurants_full[\"avg_rating\"].dropna())\n",
        "corr, p_value\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Type of test used:\n",
        "\n",
        "Pearson Correlation Test\n"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both variables are continuous (Cost_clean & avg_rating).\n",
        "\n",
        "We want to measure strength & direction of linear relationship.\n",
        "\n",
        "Data shows a roughly continuous distribution."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“Popular restaurants (high review count) have higher ratings.”\n",
        "\n",
        "Null & Alternate Hypothesis\n",
        "Null Hypothesis (H₀):\n",
        "\n",
        "There is no relationship between review count and average rating.\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "\n",
        "There is a relationship between review count and average rating."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "corr, p_value = spearmanr(restaurants_full[\"review_count\"], restaurants_full[\"avg_rating\"])\n",
        "corr, p_value\n",
        "\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spearman Rank Correlation"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review counts are highly skewed, not normally distributed.\n",
        "\n",
        "Spearman works for non-linear or non-normal relationships.\n",
        "\n",
        "Measures monotonic relationships, perfect for popularity trends."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# ----------------------------\n",
        "# HANDLING MISSING VALUES\n",
        "# ----------------------------\n",
        "\n",
        "# These are the key numeric columns we will use for clustering\n",
        "numeric_cols = [\n",
        "    \"avg_rating\",\n",
        "    \"review_count\",\n",
        "    \"total_pictures\",\n",
        "    \"avg_review_length\",\n",
        "    \"Cost_clean\"\n",
        "]\n",
        "\n",
        "# Impute missing numeric values using median\n",
        "for col in numeric_cols:\n",
        "    restaurants_full[col] = restaurants_full[col].fillna(restaurants_full[col].median())\n",
        "\n",
        "# Verify missing values\n",
        "restaurants_full[numeric_cols].isna().sum()\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your dataset contains missing Cost, missing Review Length, missing Pictures.\n",
        "\n",
        "Clustering cannot work with missing values.\n",
        "\n",
        "Median imputation maintains distribution integrity."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# ----------------------------\n",
        "# OUTLIER REMOVAL (IQR METHOD)\n",
        "# ----------------------------\n",
        "\n",
        "def iqr_filter(data, col, factor=1.5):\n",
        "    Q1 = data[col].quantile(0.25)\n",
        "    Q3 = data[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - factor * IQR\n",
        "    upper = Q3 + factor * IQR\n",
        "    return data[(data[col] >= lower) & (data[col] <= upper)]\n",
        "\n",
        "filtered_df = restaurants_full.copy()\n",
        "\n",
        "# Removing outliers in cost and review count (the two most skewed cols)\n",
        "filtered_df = iqr_filter(filtered_df, \"Cost_clean\")\n",
        "filtered_df = iqr_filter(filtered_df, \"review_count\")\n",
        "\n",
        "filtered_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(\"Before:\", restaurants_full.shape)\n",
        "print(\"After outlier removal:\", filtered_df.shape)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost and review counts have extreme values (e.g., ₹5000+, 2000+ reviews).\n",
        "\n",
        "These heavily skew clustering results.\n",
        "\n",
        "IQR method removes harmful outliers."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "#From Zomato Metadata, the only key categorical fields are:Cuisines (text) Timings (text) primary_cuisine (derived)\n",
        "\n",
        "# 1.Binary Encoding (Cuisine flags)\n",
        "\n",
        "top_cuisines_list = [\"North Indian\", \"Chinese\", \"Biryani\", \"Continental\", \"Desserts\"]\n",
        "\n",
        "for c in top_cuisines_list:\n",
        "    filtered_df[f\"cuisine_{c.replace(' ', '_').lower()}\"] = (\n",
        "        filtered_df[\"Cuisines\"]\n",
        "        .fillna(\"\")\n",
        "        .str.contains(c, case=False)\n",
        "        .astype(int)\n",
        "    )\n",
        "\n",
        "#2.Numeric Encoding for Timings (Timings Length)\n",
        "\n",
        "filtered_df[\"timings_length\"] = (\n",
        "    filtered_df[\"Timings\"]\n",
        "    .fillna(\"\")\n",
        "    .astype(str)\n",
        "    .str.len()\n",
        ")\n",
        "\n",
        "#3.Numeric Extraction (Number of Cuisines)\n",
        "\n",
        "filtered_df[\"num_cuisines\"] = (\n",
        "    filtered_df[\"Cuisines\"]\n",
        "    .fillna(\"\")\n",
        "    .apply(lambda x: len(str(x).split(\",\")))\n",
        ")\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical Encoding Techniques Used:\n",
        "\n",
        "Binary Encoding for top cuisines\n",
        "\n",
        "Used because “Cuisines” is multi-valued text, and one-hot encoding would create too many sparse columns.\n",
        "\n",
        "Text Length Encoding for ‘Timings’\n",
        "\n",
        "Converts text to a meaningful numeric feature representing operational complexity.\n",
        "\n",
        "Count Encoding for number of cuisines\n",
        "\n",
        "Represents menu diversity in numeric form.\n",
        "\n",
        "These encoding techniques keep the model efficient, avoid sparsity, and preserve business meaning."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "\n",
        "contractions = {\n",
        "    \"can't\": \"cannot\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"we're\": \"we are\"\n",
        "}\n",
        "\n",
        "contractions_pattern = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions_pattern.sub(lambda x: contractions[x.group()], text)\n",
        "\n",
        "reviews[\"Review_clean\"] = reviews[\"Review\"].astype(str).apply(expand_contractions)"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "reviews[\"Review_clean\"] = reviews[\"Review_clean\"].str.lower()\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "reviews[\"Review_clean\"] = reviews[\"Review_clean\"].apply(\n",
        "    lambda x: x.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        ")\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "\n",
        "reviews[\"Review_clean\"] = reviews[\"Review_clean\"].apply(\n",
        "    lambda x: re.sub(r\"http\\S+|www\\S+|\\d+\", \"\", x)\n",
        ")\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "reviews[\"Review_clean\"] = reviews[\"Review_clean\"].apply(\n",
        "    lambda x: \" \".join([w for w in x.split() if w not in stop_words])\n",
        ")"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "reviews[\"Review_clean\"] = reviews[\"Review_clean\"].str.strip()"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "def rephrase(text):\n",
        "    return text.replace(\"food quality\", \"quality\").replace(\"very good\", \"excellent\")\n",
        "\n",
        "reviews[\"Review_clean\"] = reviews[\"Review_clean\"].apply(rephrase)\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "reviews[\"tokens\"] = reviews[\"Review_clean\"].apply(lambda x: x.split())\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemm = WordNetLemmatizer()\n",
        "\n",
        "reviews[\"lemma_tokens\"] = reviews[\"tokens\"].apply(\n",
        "    lambda words: [lemm.lemmatize(w) for w in words]\n",
        ")\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import nltk\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
        "\n",
        "reviews[\"pos_tags\"] = reviews[\"lemma_tokens\"].apply(nltk.pos_tag)\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=500)\n",
        "tfidf_matrix = tfidf.fit_transform(reviews[\"Review_clean\"])\n",
        "\n",
        "tfidf_matrix.shape\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.TfidfVectorizer"
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# ====== FEATURE MANIPULATION ON MAIN DATASET ======\n",
        "\n",
        "\n",
        "\n",
        "# 1. Log-transform skewed numeric features\n",
        "restaurants_full[\"review_count_log\"] = np.log1p(restaurants_full[\"review_count\"])\n",
        "restaurants_full[\"cost_log\"] = np.log1p(restaurants_full[\"Cost_clean\"])\n",
        "\n",
        "# 2. Normalize review length\n",
        "restaurants_full[\"review_length_norm\"] = (\n",
        "    restaurants_full[\"avg_review_length\"] / restaurants_full[\"avg_review_length\"].max()\n",
        ")\n",
        "\n",
        "# 3. Create popularity score (weighted between reviews and pictures)\n",
        "restaurants_full[\"popularity_score\"] = (\n",
        "    restaurants_full[\"review_count\"] * 0.7 +\n",
        "    restaurants_full[\"total_pictures\"] * 0.3\n",
        ")\n",
        "\n",
        "# 4. Number of cuisines offered\n",
        "restaurants_full[\"num_cuisines\"] = (\n",
        "    restaurants_full[\"Cuisines\"].astype(str).apply(lambda x: len(x.split(\",\")))\n",
        ")\n",
        "\n",
        "# 5. Operational complexity (length of timings text)\n",
        "restaurants_full[\"timings_length\"] = restaurants_full[\"Timings\"].astype(str).str.len()\n",
        "\n",
        "# 6. Binary encode top cuisines\n",
        "top_cuisines = [\"North Indian\", \"Chinese\", \"Biryani\", \"Continental\", \"Desserts\"]\n",
        "\n",
        "for c in top_cuisines:\n",
        "    restaurants_full[f\"cuisine_{c.replace(' ', '_').lower()}\"] = (\n",
        "        restaurants_full[\"Cuisines\"].astype(str)\n",
        "        .str.contains(c, case=False)\n",
        "        .astype(int)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "\n",
        "# Select only numeric features from manipulated dataset\n",
        "numeric_df = restaurants_full.select_dtypes(include=[np.number])\n",
        "\n",
        "# 1. Variance Threshold\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "vt = VarianceThreshold(threshold=0.0)\n",
        "X_var = vt.fit_transform(numeric_df)\n",
        "\n",
        "print(\"Shape after variance threshold:\", X_var.shape)\n",
        "\n",
        "# 2. Correlation matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(numeric_df.corr(), cmap=\"coolwarm\", annot=False)\n",
        "plt.title(\"Correlation Heatmap - Numeric Features\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variance Threshold\n",
        "\n",
        "Removes features that have zero or near-zero variance.\n",
        "\n",
        "These features add no value to clustering.\n",
        "\n",
        "Correlation Analysis\n",
        "\n",
        "Helps identify redundant features (e.g., review_count vs popularity_score).\n",
        "\n",
        "Highly correlated features are removed to avoid multicollinearity.\n",
        "\n",
        " Business Understanding–Based Selection\n",
        "\n",
        "We kept only features that capture meaningful restaurant behavior such as rating, cost, popularity, cuisine diversity, and operational complexity."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature    --   \tWhy It Is Important\n",
        "\n",
        "avg_rating\t --   Reflects customer satisfaction & quality\n",
        "\n",
        "review_count_log -- Popularity indicator without skew\n",
        "\n",
        "cost_log\t     -- Cleaned,normalized restaurant pricing\n",
        "\n",
        "popularity_score --\t Weighted measure of engagement\n",
        "\n",
        "num_cuisines\t  --   Shows variety of menu\n",
        "\n",
        "timings_length\t -- Represents operational complexity\n",
        "\n",
        "cuisine flags\t  --  Distinguishes restaurant categories\n",
        "\n",
        "\n",
        "These features are essential for identifying meaningful clusters\n",
        " such as:\n",
        "low-cost high-rated, premium low-rated, multi-cuisine high popularity, etc."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the dataset required transformation.\n",
        "\n",
        "Reason 1: Skewness in review_count & Cost_clean\n",
        "Both of these columns contained extremely large values and heavy right-skewness.\n",
        "→ We applied log transformation to stabilize variance and make their distributions more normal.\n",
        "\n",
        "Reason 2: avg_review_length had inconsistent scale\n",
        "→ We applied normalization to scale text length between 0 and 1.\n",
        "\n",
        "Reason 3: Text-based fields such as Cuisines and Timings\n",
        "→ Transformed into quantitative features (num_cuisines, timings_length, cuisine flags).\n",
        "\n",
        "These transformations ensure that the final dataset is numerically consistent and suitable for distance-based clustering algorithms like KMeans and Agglomerative Clustering."
      ],
      "metadata": {
        "id": "EvysfsacYfUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(restaurants_full.select_dtypes(include=['float64', 'int64']))"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used StandardScaler for scaling the data.\n",
        "\n",
        "Reason:\n",
        "\n",
        "Clustering algorithms (KMeans, Agglomerative) use distance calculations.\n",
        "\n",
        "Features like Cost_clean, review_count, popularity_score were much larger in scale than ratings (0–5).\n",
        "\n",
        "StandardScaler normalizes all features to:"
      ],
      "metadata": {
        "id": "egwKlsPLY2VX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here. Yes, dimensionality reduction was needed.\n",
        "\n",
        "Reasons:\n",
        "\n",
        "Feature engineering created many new numerical variables:\n",
        "\n",
        "log features\n",
        "\n",
        "cuisine flags\n",
        "\n",
        "popularity score\n",
        "\n",
        "diversity score\n",
        "\n",
        "timings_length\n",
        "\n",
        "High-dimensional feature space reduces cluster separability.\n",
        "\n",
        "PCA helps remove noise and improve visualization.\n",
        "\n",
        "Thus PCA enhances both model stability and interpretability."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here. I used Principal Component Analysis (PCA) because:\n",
        "\n",
        "It reduces multicollinearity\n",
        "\n",
        "Helps visualize clusters in 2D\n",
        "\n",
        "Retains maximum variance\n",
        "\n",
        "Improves computational efficiency\n",
        "\n",
        "Helps interpret cluster boundaries clearly\n",
        "\n",
        "PCA was ideal for compressing engineered features into meaningful components."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data splitting is not required because this is an unsupervised learning (clustering) problem.\n",
        "\n",
        "There is no target label.\n",
        "\n",
        "We are not predicting anything; we are grouping restaurants.\n",
        "\n",
        "The entire dataset is used for cluster formation.\n",
        "\n",
        "Thus, data splitting (train/test) is not applicable."
      ],
      "metadata": {
        "id": "C2c21tKNZTZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here. Not Required here"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.No, the dataset is not imbalanced.\n",
        "\n",
        "Imbalance only exists when you have class labels with unequal distribution.\n",
        "\n",
        "Since clustering has no labels, the concept of imbalance does not apply."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.No imbalance handling technique was required.\n",
        "\n",
        "Methods like SMOTE, undersampling, oversampling apply only to classification problems.\n",
        "\n",
        "Clustering works on unlabeled data, so balancing is unnecessary."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1  ---- **Implementation (KMeans)**"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# ==============================\n",
        "# ML Model - 1 : KMEANS CLUSTERING\n",
        "# ==============================\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# 1. Choose an initial number of clusters (based on elbow / domain guess)\n",
        "initial_k = 4\n",
        "\n",
        "# 2. Fit the KMeans model\n",
        "kmeans_model = KMeans(n_clusters=initial_k, random_state=42)\n",
        "kmeans_labels = kmeans_model.fit_predict(X_scaled)\n",
        "\n",
        "# 3. Attach cluster labels to main dataset\n",
        "restaurants_full[\"kmeans_cluster\"] = kmeans_labels\n",
        "\n",
        "# 4. Evaluate using Silhouette Score (internal clustering metric)\n",
        "sil_score = silhouette_score(X_scaled, kmeans_labels)\n",
        "print(f\"Initial KMeans model with k={initial_k}\")\n",
        "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
        "\n",
        "# 5. Quick look at cluster sizes\n",
        "print(\"\\nCluster counts:\")\n",
        "print(restaurants_full[\"kmeans_cluster\"].value_counts())\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For ML Model – 1, I used KMeans clustering.\n",
        "KMeans is an unsupervised learning algorithm that partitions the data into K clusters by minimizing the sum of squared distances between points and their assigned cluster centers.\n",
        "\n",
        "To evaluate the model, I used two internal clustering metrics:\n",
        "\n",
        "Inertia / SSE (Sum of Squared Errors) → measures how compact the clusters are (lower is better).\n",
        "\n",
        "Silhouette Score → measures how well samples are assigned to their clusters vs other clusters (closer to 1 is better, 0 means overlapping, negative is bad).\n",
        "\n",
        "After training KMeans with an initial choice of k = 4, I computed the Silhouette Score.\n",
        "This score provides an internal measure of clustering quality and is visualized against different values of K in the evaluation metric score chart (Elbow + Silhouette plots)."
      ],
      "metadata": {
        "id": "RlhpoivIcABr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# ==============================================\n",
        "# EVALUATION METRIC SCORE CHART (SSE + SILHOUETTE)\n",
        "# ==============================================\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "sse = []\n",
        "sil_scores = []\n",
        "K_range = range(2, 11)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "    sse.append(kmeans.inertia_)\n",
        "    sil_scores.append(silhouette_score(X_scaled, labels))\n",
        "\n",
        "# Plot 1: Elbow (SSE vs k)\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(K_range, sse, marker='o')\n",
        "plt.title(\"Elbow Method - SSE vs Number of Clusters (k)\")\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"SSE (Inertia)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: Silhouette Score vs k\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(K_range, sil_scores, marker='o')\n",
        "plt.title(\"Silhouette Score vs Number of Clusters (k)\")\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "# ======================================\n",
        "# KMEANS HYPERPARAMETER OPTIMIZATION\n",
        "# (choosing best k using Silhouette Score)\n",
        "# ======================================\n",
        "\n",
        "best_k = None\n",
        "best_sil = -1\n",
        "silhouette_per_k = {}\n",
        "\n",
        "for k in range(2, 11):\n",
        "    kmeans_temp = KMeans(n_clusters=k, random_state=42)\n",
        "    labels_temp = kmeans_temp.fit_predict(X_scaled)\n",
        "    sil_temp = silhouette_score(X_scaled, labels_temp)\n",
        "    silhouette_per_k[k] = sil_temp\n",
        "\n",
        "    if sil_temp > best_sil:\n",
        "        best_sil = sil_temp\n",
        "        best_k = k\n",
        "\n",
        "print(\"Silhouette score for each k:\")\n",
        "for k, s in silhouette_per_k.items():\n",
        "    print(f\"k={k}: Silhouette Score = {s:.4f}\")\n",
        "\n",
        "print(\"\\nBest k based on Silhouette Score:\", best_k)\n",
        "print(\"Best Silhouette Score:\", best_sil)\n",
        "\n",
        "# Fit final optimized KMeans model\n",
        "kmeans_opt = KMeans(n_clusters=best_k, random_state=42)\n",
        "opt_labels = kmeans_opt.fit_predict(X_scaled)\n",
        "\n",
        "restaurants_full[\"kmeans_cluster_opt\"] = opt_labels\n",
        "\n",
        "print(\"\\nOptimized KMeans cluster counts:\")\n",
        "print(restaurants_full[\"kmeans_cluster_opt\"].value_counts())\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "For KMeans, the most important hyperparameter is the number of clusters (k).\n",
        "I used a grid-search style approach over k (from 2 to 10), evaluating each model using the Silhouette Score.\n",
        "\n",
        "This is similar to GridSearchCV, but instead of a labeled validation set, I used an internal clustering validity index (Silhouette Score) as the objective function.\n",
        "I selected the k which maximized the Silhouette Score as the optimal hyperparameter value.\n",
        "\n",
        "This method is appropriate for unsupervised learning where traditional cross-validation with labels is not possible.\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.Yes, there was an improvement after hyperparameter optimization.\n",
        "\n",
        "Initially, I chose k = 4 based on a visual inspection of the Elbow curve.\n",
        "After performing Silhouette-based hyperparameter search over k = 2 to 10, I selected the value of k that achieved the highest Silhouette Score.\n",
        "\n",
        "The improvement was:\n",
        "\n",
        "Before tuning: Silhouette Score for initial k ( k = 3)\n",
        "\n",
        "After tuning: Silhouette Score for best_k (chosen by the search)\n",
        "\n",
        "The evaluation metric score chart (Silhouette Score vs k) clearly shows that the chosen best_k corresponds to a local maximum in Silhouette Score, which means better cluster separation and more meaningful grouping of restaurants."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 Implementation (Agglomerative Clustering)"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# ML Model - 2 : AGGLOMERATIVE CLUSTERING (FIXED)\n",
        "# ==============================\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "initial_k_agg = 4  # same as KMeans for comparison\n",
        "\n",
        "agg_model = AgglomerativeClustering(\n",
        "    n_clusters=initial_k_agg,\n",
        "    metric=\"euclidean\",   # FIXED (replaces affinity)\n",
        "    linkage=\"ward\"\n",
        ")\n",
        "\n",
        "agg_labels = agg_model.fit_predict(X_scaled)\n",
        "\n",
        "restaurants_full[\"agg_cluster\"] = agg_labels\n",
        "\n",
        "sil_agg = silhouette_score(X_scaled, agg_labels)\n",
        "print(f\"Agglomerative Clustering with k={initial_k_agg}\")\n",
        "print(f\"Silhouette Score: {sil_agg:.4f}\")\n",
        "\n",
        "print(\"\\nCluster counts:\")\n",
        "print(restaurants_full[\"agg_cluster\"].value_counts())\n"
      ],
      "metadata": {
        "id": "CsLzOiIBdYnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML Model – 2 is Agglomerative (Hierarchical) Clustering.\n",
        "Unlike KMeans, which partitions data by optimizing the distance to centroids, Agglomerative Clustering is a bottom-up hierarchical algorithm.\n",
        "It starts with each point as its own cluster and iteratively merges the closest pairs of clusters until the desired number of clusters is reached.\n",
        "\n",
        "I used:\n",
        "\n",
        "Euclidean distance as the similarity metric\n",
        "\n",
        "Ward linkage, which minimizes variance within clusters at each merge step\n",
        "\n",
        "I evaluated the clustering quality using Silhouette Score and compared its performance against KMeans. Having two different clustering models helps validate whether the cluster structure is stable and robust."
      ],
      "metadata": {
        "id": "7H0H6XEdd383"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# =======================================================\n",
        "# AGGLOMERATIVE CLUSTERING - EVALUATION METRIC SCORE CHART\n",
        "# =======================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "K_range = range(2, 11)\n",
        "sil_scores_agg = []\n",
        "\n",
        "for k in K_range:\n",
        "    agg_temp = AgglomerativeClustering(\n",
        "        n_clusters=k,\n",
        "        metric=\"euclidean\",\n",
        "        linkage=\"ward\"\n",
        "    )\n",
        "    labels_temp = agg_temp.fit_predict(X_scaled)\n",
        "    sil_temp = silhouette_score(X_scaled, labels_temp)\n",
        "    sil_scores_agg.append(sil_temp)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(K_range, sil_scores_agg, marker='o')\n",
        "plt.title(\"Agglomerative Clustering - Silhouette Score vs Number of Clusters (k)\")\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "# =====================================================\n",
        "# AGGLOMERATIVE CLUSTERING - HYPERPARAMETER OPTIMIZATION\n",
        "# (SEARCH OVER NUMBER OF CLUSTERS USING SILHOUETTE SCORE)\n",
        "# =====================================================\n",
        "\n",
        "best_k_agg = None\n",
        "best_sil_agg = -1\n",
        "silhouette_per_k_agg = {}\n",
        "\n",
        "for k in range(2, 11):\n",
        "    agg_temp = AgglomerativeClustering(\n",
        "        n_clusters=k,\n",
        "        metric=\"euclidean\",\n",
        "        linkage=\"ward\"\n",
        "    )\n",
        "    labels_temp = agg_temp.fit_predict(X_scaled)\n",
        "    sil_temp = silhouette_score(X_scaled, labels_temp)\n",
        "    silhouette_per_k_agg[k] = sil_temp\n",
        "\n",
        "    if sil_temp > best_sil_agg:\n",
        "        best_sil_agg = sil_temp\n",
        "        best_k_agg = k\n",
        "\n",
        "print(\"Silhouette score for each k (Agglomerative):\")\n",
        "for k, s in silhouette_per_k_agg.items():\n",
        "    print(f\"k={k}: Silhouette Score = {s:.4f}\")\n",
        "\n",
        "print(\"\\nBest k (Agglomerative) based on Silhouette Score:\", best_k_agg)\n",
        "print(\"Best Silhouette Score (Agglomerative):\", best_sil_agg)\n",
        "\n",
        "# Fit final optimized Agglomerative model\n",
        "agg_opt = AgglomerativeClustering(\n",
        "    n_clusters=best_k_agg,\n",
        "    metric=\"euclidean\",\n",
        "    linkage=\"ward\"\n",
        ")\n",
        "agg_opt_labels = agg_opt.fit_predict(X_scaled)\n",
        "\n",
        "restaurants_full[\"agg_cluster_opt\"] = agg_opt_labels\n",
        "\n",
        "print(\"\\nOptimized Agglomerative Cluster counts:\")\n",
        "print(restaurants_full[\"agg_cluster_opt\"].value_counts())\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.For Agglomerative Clustering, the most important hyperparameter is the number of clusters (n_clusters).\n",
        "I used a grid search–style hyperparameter optimization where I trained the model for different values of k (from 2 to 10) and computed the Silhouette Score for each.\n",
        "\n",
        "This approach is similar to GridSearchCV, but adapted to unsupervised learning because we do not have labels.\n",
        "The model with the highest Silhouette Score was selected as the best configuration (best_k_agg).\n",
        "\n",
        "This is appropriate because Silhouette Score is a standard internal metric to evaluate how well instances are clustered."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.Yes, there was improvement after hyperparameter optimization for Agglomerative Clustering.\n",
        "\n",
        "Initially, I fixed n_clusters = 4 (to match KMeans and based on Elbow intuition).\n",
        "After running a Silhouette-based search over k = 2 to 10, I selected the value of k that produced the maximum Silhouette Score.\n",
        "\n",
        "The updated evaluation score chart (Silhouette Score vs k for Agglomerative) clearly shows that the chosen best_k_agg corresponds to a local maximum, which indicates:\n",
        "\n",
        "Better separation between clusters\n",
        "\n",
        "More coherent grouping of restaurants\n",
        "\n",
        "I then refit the model with best_k_agg and updated the cluster labels in the dataset as agg_cluster_opt."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here. Metric ---\tBusiness Impact  \n",
        "\n",
        "SSE/Inertia\tMeasures ----compactness → meaningful groups\n",
        "\n",
        "Silhouette Score\t---Measures separation → reliable segmentation\n",
        "\n",
        "Elbow Method\t---- optimal number of clusters\n",
        "\n",
        "Dendrogram----\tReveals structure, competition, and hidden segments"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here. I considered three main evaluation metrics because they directly connect to business outcomes and decision-making:\n",
        "\n",
        "1️⃣ Silhouette Score (Most Important for Business)\n",
        "\n",
        "Why?\n",
        "Silhouette Score measures how well restaurants fit within their assigned cluster and how different each cluster is from others.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Helps Zomato identify clear, distinct customer/restaurant segments\n",
        "\n",
        "Enables accurate recommendation systems\n",
        "\n",
        "Reduces overlap between segments (e.g., premium vs budget)\n",
        "\n",
        "Ensures that the segmentation strategy is reliable and actionable\n",
        "\n",
        "This metric gives the strongest indication of how meaningful and business-ready the clusters are.\n",
        "\n",
        "2️⃣ SSE / Inertia (Cluster Compactness)\n",
        "\n",
        "Why?\n",
        "Measures how similar restaurants inside each cluster are.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Ensures each cluster represents a homogeneous group\n",
        "\n",
        "Makes market segmentation stable\n",
        "\n",
        "Helps identify cohesive categories for targeted marketing\n",
        "\n",
        "A lower SSE = stronger, more useful clusters for business decisions.\n",
        "\n",
        "3️⃣ Elbow Method (Choosing Optimal K)\n",
        "\n",
        "Why?\n",
        "The elbow point gives the perfect balance between too many clusters (over-segmentation) and too few clusters (information loss).\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Prevents unnecessary complexity in business strategy\n",
        "\n",
        "Ensures optimal segmentation for marketing campaigns\n",
        "\n",
        "Helps Zomato work with a realistic number of restaurant sectors"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.I selected the KMeans Clustering Model as the final model.\n",
        "\n",
        "✔ Reasons for choosing KMeans over Agglomerative:\n",
        "\n",
        "Higher Silhouette Score\n",
        "KMeans produced better cluster separation and more meaningful segments.\n",
        "\n",
        "More scalable for large datasets\n",
        "Zomato’s real data is huge (>100k restaurants).\n",
        "KMeans works efficiently even at scale.\n",
        "Agglomerative becomes slow and memory-heavy.\n",
        "\n",
        "More stable and repeatable\n",
        "With random_state=42, KMeans gives consistent clusters.\n",
        "Agglomerative changes structure depending on merge steps.\n",
        "\n",
        "Produces business-friendly clusters\n",
        "KMeans created clusters that clearly matched real-world segments like:\n",
        "\n",
        "Premium fine dining\n",
        "\n",
        "Budget-friendly restaurants\n",
        "\n",
        "Highly popular fast food outlets\n",
        "\n",
        "Low-rated niche restaurants\n",
        "\n",
        "Easy to interpret with PCA\n",
        "KMeans clusters looked clearer and more separable in PCA scatter plots."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.Since clustering is unsupervised, traditional feature importance does not apply.\n",
        "BUT we can still explain the model using:\n",
        "\n",
        "✔ 1. KMeans Cluster Centroids → Feature Influence\n",
        "\n",
        "We can analyze cluster centers to understand which features play the biggest role.\n",
        "\n",
        "✔ 2. SHAP for Clustering (Cluster Explainer)\n",
        "\n",
        "We can use PCA + SHAP to interpret how features drive cluster formation."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here.\n",
        "\n",
        "The objective of this project was to analyze Zomato restaurant data and segment restaurants into meaningful groups using unsupervised machine learning techniques. Through extensive data cleaning, feature engineering, exploratory analysis, scaling, dimensionality reduction, and clustering using KMeans and Agglomerative Clustering, the project successfully uncovered clear and actionable restaurant segments.\n",
        "\n",
        "KMeans emerged as the best-performing model due to its higher Silhouette Score, better cluster separation, and more interpretable groupings. The assigned clusters revealed distinct business personas such as premium fine-dining restaurants, budget-friendly popular outlets, multi-cuisine family restaurants, and low-engagement niche restaurants. These segments were validated through evaluation metrics like SSE, Silhouette Score, Elbow Method, and Hierarchical dendrograms.\n",
        "\n",
        "The insights derived from clustering carry significant business value. Zomato can use these segments to improve personalized recommendations, optimize marketing campaigns, identify restaurants needing operational or rating improvements, understand cuisine trends, and enhance user engagement across the platform. From a partner perspective, cluster insights help restaurants understand where they stand in the competitive landscape and how they can improve pricing, menu diversity, and customer engagement.\n",
        "\n",
        "Finally, the model was saved and tested on unseen data for deployability, proving that the clustering system can be integrated into real-time applications such as recommendation engines, campaign targeting systems, and market research dashboards.\n",
        "\n",
        "In conclusion, this project demonstrates how unsupervised machine learning can transform raw restaurant data into valuable business intelligence, enabling Zomato to make data-driven decisions that benefit customers, restaurant partners, and the platform’s overall growth."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}